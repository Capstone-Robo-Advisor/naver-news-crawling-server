import sys
import os
import json
from datetime import datetime, timedelta

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from fastapi import FastAPI, HTTPException
from playwright.async_api import async_playwright, TimeoutError

app = FastAPI()

# ê¸°ë³¸ URL
BASE_URL = "https://news.naver.com/breakingnews/section/101/258"

async def crawl_news() -> list[dict[str, str | None]] | None:
    articles = []
    target_article_count = 200  # ëª©í‘œ ê¸°ì‚¬ ìˆ˜

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        context = await browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
        )
        
        page = await context.new_page()
        
        # ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ
        await page.goto(BASE_URL)
        await page.wait_for_selector("div.sa_item_inner", timeout=20000)
        print("ì´ˆê¸° í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
        # ì´ˆê¸° ê¸°ì‚¬ ìˆ˜ì§‘
        current_articles = await collect_articles_from_page(page)
        articles.extend(current_articles)
        print(f"ì´ˆê¸° ê¸°ì‚¬ ìˆ˜: {len(articles)}")
        
        # ëª©í‘œ ê¸°ì‚¬ ìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ 'ê¸°ì‚¬ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­
        more_button_selector = "#newsct > div.section_latest > div > div.section_more > a"
        click_count = 0
        max_clicks = 30  # ìµœëŒ€ í´ë¦­ íšŸìˆ˜ ì œí•œ
        
        while len(articles) < target_article_count and click_count < max_clicks:
            try:
                # í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
                await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                await page.wait_for_timeout(1000)
                
                # 'ê¸°ì‚¬ ë”ë³´ê¸°' ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                is_button_visible = await page.is_visible(more_button_selector)
                if not is_button_visible:
                    print("ë” ì´ìƒ 'ê¸°ì‚¬ ë”ë³´ê¸°' ë²„íŠ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # ë²„íŠ¼ í´ë¦­
                print(f"'ê¸°ì‚¬ ë”ë³´ê¸°' ë²„íŠ¼ í´ë¦­ ì‹œë„ {click_count + 1}")
                await page.click(more_button_selector)
                await page.wait_for_timeout(3000)  # ë°ì´í„° ë¡œë“œ ëŒ€ê¸°
                
                # ìƒˆ ê¸°ì‚¬ ìˆ˜ì§‘
                new_articles = await collect_articles_from_page(page)
                
                # ì¤‘ë³µ ì œê±°
                before_count = len(articles)
                for article in new_articles:
                    if not any(existing["link"] == article["link"] for existing in articles):
                        articles.append(article)
                
                new_added = len(articles) - before_count
                print(f"í´ë¦­ {click_count + 1} í›„ {new_added}ê°œ ìƒˆ ê¸°ì‚¬ ì¶”ê°€ë¨. í˜„ì¬ ì´ {len(articles)}ê°œ")
                
                # ìƒˆ ê¸°ì‚¬ê°€ ì¶”ê°€ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¢…ë£Œ
                if new_added == 0:
                    consecutive_no_new += 1
                    if consecutive_no_new >= 3:  # 3ë²ˆ ì—°ì† ìƒˆ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
                        print("3ë²ˆ ì—°ì† ìƒˆ ê¸°ì‚¬ê°€ ì¶”ê°€ë˜ì§€ ì•Šì•„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                else:
                    consecutive_no_new = 0
                
                click_count += 1
                
            except TimeoutError:
                print(f"ë²„íŠ¼ í´ë¦­ ì‹œ íƒ€ì„ì•„ì›ƒ ë°œìƒ (ì‹œë„ {click_count + 1})")
                break
            except Exception as e:
                print(f"ë²„íŠ¼ í´ë¦­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                break
                
        await browser.close()
        print(f"ìµœì¢… í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(articles)}ê°œ ê¸°ì‚¬")
        return articles

async def collect_articles_from_page(page) -> list[dict[str, str | None]]:
    """í˜„ì¬ í˜ì´ì§€ì—ì„œ ëª¨ë“  ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    articles = []
    
    # ê¸°ì‚¬ ìˆ˜ì§‘
    news_items = await page.query_selector_all("div.sa_item_inner")
    for news in news_items:
        try:
            # ì¸ë„¤ì¼
            thumbnail_elem = await news.query_selector(".sa_thumb_inner img")
            thumbnail = await thumbnail_elem.get_attribute('src') if thumbnail_elem else None

            # ë§í¬
            link_elem = await news.query_selector(".sa_thumb_link")
            link = await link_elem.get_attribute('href') if link_elem else None

            # ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
            title_elem = await news.query_selector(".sa_text_strong")
            title = await title_elem.inner_text() if title_elem else None

            # ìš”ì•½ í¬ë¡¤ë§
            lede_elem = await news.query_selector(".sa_text_lede")
            lede = await lede_elem.inner_text() if lede_elem else None

            # ì¶œì²˜
            source_elem = await news.query_selector(".sa_text_press")
            source = await source_elem.inner_text() if source_elem else None

            # ëª‡ ì‹œê°„ ì „ì¸ì§€
            time_elem = await news.query_selector(".sa_text_datetime b")
            time = await time_elem.inner_text() if time_elem else None
            
            articles.append({
                "thumbnail": thumbnail,
                "link": link,
                "title": title,
                "lede": lede,
                "source": source,
                "time": time
            })
        except Exception as e:
            print(f"ê¸°ì‚¬ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            continue
    
    return articles

def save_to_json(data: list[dict[str, str | None]]):
    '''
    í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    '''
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"news_{timestamp}.json"

    # JSON ë°ì´í„° ì €ì¥
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“‹ ë°ì´í„°ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ : {filename}")

@app.get("/news")
async def crawl_news_endpoint():
    '''
    ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ API ì—”ë“œí¬ì¸íŠ¸
    '''
    try:
        articles = await crawl_news()

        # ë°ì´í„°ê°€ ìˆìœ¼ë©´ JSON íŒŒì¼ë¡œ ì €ì¥
        if articles:
            save_to_json(articles)
            return {
                "message": f"í¬ë¡¤ë§ ì„±ê³µ ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ (ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘)",
                "total_articles": len(articles),
                "data": articles
            }
        else:
            return {"message" : "í¬ë¡¤ë§ ì„±ê³µ (ê·¸ëŸ¬ë‚˜ ë°ì´í„° ì—†ìŒ)", "data" : []}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))