import sys
import os
import json

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from datetime import datetime
from fastapi import FastAPI, HTTPException
from playwright.async_api import async_playwright

app = FastAPI()

BASE_URL = "https://news.naver.com/breakingnews/section/101/258"

async def crawl_news() -> list[dict[str, str | None]] | None:
    articles = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)  # `await` ì¶”ê°€
        page = await browser.new_page()

        # User-Agent ì¶”ê°€
        await page.set_extra_http_headers({
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36"
        })

        await page.goto(BASE_URL)

        # AJAX ë°ì´í„° ë¡œë“œ ëŒ€ê¸°
        await page.wait_for_selector("div.sa_item_inner", timeout=20000)

        # ìŠ¤í¬ë¡¤ì„ í†µí•´ ëª¨ë“  ê¸°ì‚¬ ë¡œë“œ
        last_height = 0
        while True:
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await page.wait_for_timeout(5000)  # 5ì´ˆ ëŒ€ê¸°
            new_height = await page.evaluate("document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height

        # ê¸°ì‚¬ ìˆ˜ì§‘
        for news in await page.query_selector_all(".sa_item_inner"):
            try:
                # ì¸ë„¤ì¼
                thumbnail_elem = await news.query_selector(".sa_thumb_inner img")
                thumbnail = await thumbnail_elem.get_attribute('src') if thumbnail_elem else None

                # ë§í¬
                link_elem = await news.query_selector(".sa_thumb_link")
                link = await link_elem.get_attribute('href') if link_elem else None

                # ê¸°ì‚¬ ì œëª© í¬ë¡¤ë§
                title_elem = await news.query_selector(".sa_text_strong")
                title = await title_elem.inner_text() if title_elem else None

                # ìš”ì•½ í¬ë¡¤ë§
                lede_elem = await news.query_selector(".sa_text_lede")
                lede = await lede_elem.inner_text() if lede_elem else None

                # ì¶œì²˜
                source_elem = await news.query_selector(".sa_text_press")
                source = await source_elem.inner_text() if source_elem else None

                # ëª‡ ì‹œê°„ ì „ì¸ì§€
                time_elem = await news.query_selector(".sa_text_datetime b")
                time = await time_elem.inner_text() if time_elem else None
                articles.append({
                    "thumbnail": thumbnail,
                    "link": link,
                    "title": title,
                    "lede": lede,
                    "source": source,
                    "time": time
                })
            except AttributeError:
                continue

        await browser.close()
        return articles

def save_to_json(data: list[dict[str, str | None]]):
    '''
    í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    '''
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    filename = f"news_{timestamp}.json"

    # JSON ë°ì´í„° ì €ì¥
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“‹ ë°ì´í„°ê°€ JSON íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ : {filename}")

@app.get("/news")
async def crawl_news_endpoint():
    '''
    ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ API ì—”ë“œí¬ì¸íŠ¸
    '''
    try:
        articles = await crawl_news()

        # ë°ì´í„°ê°€ ìˆìœ¼ë©´ JSON íŒŒì¼ë¡œ ì €ì¥
        if articles:
            save_to_json(articles)
            return {
                "message": f"í¬ë¡¤ë§ ì„±ê³µ ë° ë°ì´í„° ì €ì¥ ì™„ë£Œ (ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘)",
                "total_articles": len(articles),
                "data": articles
            }
        else:
            return {"message" : "í¬ë¡¤ë§ ì„±ê³µ (ê·¸ëŸ¬ë‚˜ ë°ì´í„° ì—†ìŒ)", "data" : []}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))